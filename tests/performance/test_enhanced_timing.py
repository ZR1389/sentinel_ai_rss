#!/usr/bin/env python3
"""
Test Enhanced Timing in Chat Handler
Tests the new detailed timing tracking for performance monitoring and optimization
"""
import os
import sys
import time
import json
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add parent directories to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

def test_timing_tracking():
    """Test the enhanced timing tracking in handle_user_query"""
    print("ğŸ§ª Testing Enhanced Timing Tracking...")
    print("=" * 60)
    
    try:
        from chat_handler import handle_user_query
        
        print("âœ… Chat handler imported successfully")
        print("")
        
        # Test timing with a simple query
        print("ğŸ“‹ Test: Timing Tracking with Simple Query")
        test_email = "timing.test@example.com"
        test_query = "What are the cybersecurity threats in San Francisco?"
        
        print(f"   ğŸ“ Query: {test_query}")
        print(f"   ğŸ‘¤ User: {test_email}")
        print("")
        
        # Capture timing output by monitoring logs
        print("ğŸ• Executing query with detailed timing...")
        
        # Set up logging to capture timing info
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        start_time = time.time()
        
        try:
            result = handle_user_query(
                message=test_query,
                email=test_email,
                body={"profile_data": {"role": "Security Analyst"}}
            )
            
            execution_time = time.time() - start_time
            print(f"âœ… Query completed in {execution_time:.2f}s")
            
            # Check result structure
            if isinstance(result, dict):
                print(f"   ğŸ“Š Result keys: {list(result.keys())}")
                if result.get("reply"):
                    print(f"   ğŸ’¬ Reply preview: {result['reply'][:100]}...")
                if result.get("alerts"):
                    print(f"   ğŸš¨ Alerts count: {len(result['alerts'])}")
            
            print("")
            print("ğŸ” Expected Log Output Should Include:")
            print("   âœ… Setup phase: X.XXXs")
            print("   âœ… DB phase: X.XXXs (N alerts)")
            print("   âœ… Preprocessing phase: X.XXXs") 
            print("   âœ… Advisor phase: X.XXXs")
            print("   âœ… === TIMING SUMMARY ===")
            print("   âœ… Total request time: X.XXXs")
            print("   âœ… === END TIMING ===")
            
        except Exception as e:
            print(f"âŒ Query failed: {e}")
            # Even on failure, timing should be logged
            
    except ImportError as e:
        print(f"   âŒ Import failed: {e}")
        print("   ğŸ’¡ Make sure chat_handler.py is accessible")
    except Exception as e:
        print(f"   âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()

def test_timing_benefits():
    """Show the benefits of enhanced timing tracking"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Enhanced Timing Tracking Benefits")
    print("=" * 60)
    
    timing_improvements = [
        ("Performance Monitoring", "âšª No detailed breakdown", "ğŸ“Š Phase-by-phase timing"),
        ("Bottleneck Identification", "ğŸ¤· Hard to find slow parts", "ğŸ¯ Pinpoint exact phase"),
        ("LLM Optimization", "â“ Unknown provider performance", "âš¡ Track Grokâ†’OpenAIâ†’Moonshotâ†’DeepSeek"),
        ("DB Query Monitoring", "â±ï¸ Basic timing", "ğŸ” Slow query detection (>20s)"),
        ("Advisor Performance", "ğŸ•’ Simple duration", "ğŸ“ˆ Slow advisor alerts (>60s)"),
        ("Overall Request Health", "â° Total time only", "ğŸš¨ Slow request alerts (>50s)"),
        ("Log Structure", "ğŸ“ Scattered timing", "ğŸ“‹ Organized timing summary"),
        ("Security Events", "ğŸ” Basic logging", "ğŸ›¡ï¸ Performance-based security events")
    ]
    
    print(f"{'Aspect':<25} {'Before (Basic)':<25} {'After (Enhanced)':<30}")
    print("-" * 80)
    for aspect, before, after in timing_improvements:
        print(f"{aspect:<25} {before:<25} {after:<30}")
    
    print("\nğŸ¯ Key Timing Features:")
    print("   ğŸ• Setup phase - User validation, cache checks")
    print("   ğŸ’¾ DB phase - Alert fetching with slow query detection")  
    print("   ğŸ”„ Preprocessing - Geographic analysis, historical context")
    print("   ğŸ¤– Advisor phase - LLM calls with provider priority")
    print("   ğŸ“Š Organized summary - Clear phase breakdown")
    print("   ğŸš¨ Performance alerts - Security events for slow operations")

def test_performance_thresholds():
    """Test the performance threshold system"""
    print("\n" + "=" * 60)
    print("â±ï¸ Performance Threshold Analysis")
    print("=" * 60)
    
    thresholds = {
        "DB Query": {"threshold": 20, "action": "slow_db_query security event"},
        "Advisor Call": {"threshold": 60, "action": "slow_advisor_call security event"},
        "Total Request": {"threshold": 50, "action": "slow_request security event with breakdown"}
    }
    
    print("ğŸ¯ Performance Monitoring Thresholds:")
    for phase, config in thresholds.items():
        print(f"   ğŸ“Š {phase}: >{config['threshold']}s â†’ {config['action']}")
    
    print("\nğŸ” Example Security Event for Slow Request:")
    example_event = {
        "event_type": "slow_request",
        "email": "user@example.com",
        "plan": "FREE", 
        "details": "Total: 65.23s (Setup: 0.12s, DB: 45.67s, Preprocessing: 2.34s, Advisor: 17.10s)"
    }
    print(json.dumps(example_event, indent=2))
    
    print("\nâœ… Benefits for Operations:")
    print("   ğŸ”§ Identify which phase is causing slowdowns")
    print("   ğŸ“ˆ Monitor LLM provider performance over time")
    print("   ğŸ› ï¸ Optimize the slowest components first")
    print("   ğŸ“Š Track improvements after optimizations")
    print("   ğŸš¨ Alert on performance degradation")

def test_llm_provider_timing_insights():
    """Show how timing helps with LLM provider optimization"""
    print("\n" + "=" * 60)
    print("ğŸ¤– LLM Provider Performance Insights")
    print("=" * 60)
    
    print("ğŸ“Š With Enhanced Timing, You Can Track:")
    
    provider_scenarios = [
        {
            "scenario": "Grok Fast Response",
            "advisor_time": "8.23s",
            "insight": "âœ… Primary provider working well"
        },
        {
            "scenario": "Grok Timeout â†’ OpenAI Fallback", 
            "advisor_time": "35.67s",
            "insight": "âš ï¸ Grok slow, fallback working"
        },
        {
            "scenario": "All Providers Slow",
            "advisor_time": "78.45s", 
            "insight": "ğŸš¨ System-wide LLM issues"
        },
        {
            "scenario": "DeepSeek Free Tier Hit",
            "advisor_time": "12.34s",
            "insight": "ğŸ¯ Paid providers exhausted, using free tier"
        }
    ]
    
    for scenario in provider_scenarios:
        print(f"\nğŸ¬ {scenario['scenario']}:")
        print(f"   â±ï¸ Advisor phase: {scenario['advisor_time']}")
        print(f"   ğŸ’¡ Insight: {scenario['insight']}")
    
    print("\nğŸ”§ Optimization Actions Based on Timing:")
    print("   ğŸ“ˆ Advisor >60s â†’ Check LLM provider order")
    print("   ğŸ’¾ DB >20s â†’ Optimize database queries")
    print("   ğŸ”„ Preprocessing >10s â†’ Improve geographic processing")
    print("   ğŸ• Setup >5s â†’ Review cache efficiency")
    
    print("\nâœ… Your Grokâ†’OpenAIâ†’Moonshotâ†’DeepSeek priority is now trackable!")

if __name__ == "__main__":
    print("ğŸ§ª Sentinel AI Enhanced Timing Tracking Test")
    print("Testing detailed performance monitoring and bottleneck identification")
    
    test_timing_tracking()
    test_timing_benefits()
    test_performance_thresholds()
    test_llm_provider_timing_insights()
    
    print("\n" + "=" * 60)
    print("âœ… Enhanced Timing Tracking Analysis Complete!")
    print("ğŸ“ Your timing improvements provide excellent performance visibility!")
