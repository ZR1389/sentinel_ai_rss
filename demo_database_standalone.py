#!/usr/bin/env python3
"""
Enhanced Database Operations - Standalone Demo
==============================================

This demo showcases the enhanced database operations features
without requiring actual database connectivity.

Features demonstrated:
1. Query performance monitoring and logging concepts
2. Database optimization strategies
3. Index analysis principles
4. Health monitoring frameworks
"""

import time
import hashlib
import logging
from datetime import datetime
from typing import Dict, Any, List

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger("db_demo")

# Simulated query performance tracking
_query_stats = {}
_slow_query_threshold = 1.0

def print_header(title):
    """Print formatted header"""
    print(f"\n{'='*80}")
    print(f"ğŸ”§ {title}")
    print(f"{'='*80}")

def print_section(title):
    """Print formatted section"""
    print(f"\n{'ğŸ“Š ' + title}")
    print(f"{'-'*60}")

def sanitize_query_for_log(query: str, max_length: int = 150) -> str:
    """Sanitize query for logging (remove sensitive data, truncate)"""
    import re
    sanitized = query
    
    # Replace potential sensitive values in common patterns
    sanitized = re.sub(r'(password|token|key)\s*=\s*[\'"][^\'\"]+[\'"]', r'\1=***', sanitized, flags=re.IGNORECASE)
    sanitized = re.sub(r'(password|token|key)\s*=\s*\%s', r'\1=%s', sanitized, flags=re.IGNORECASE)
    
    # Truncate if too long
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length] + "..."
    
    return sanitized

def log_query_performance(query: str, params: tuple, duration: float, row_count: int = None):
    """Log database query performance metrics"""
    # Create query signature for tracking
    query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
    query_type = query.strip().split()[0].upper()
    
    # Track query statistics
    if query_hash not in _query_stats:
        _query_stats[query_hash] = {
            'query_type': query_type,
            'total_calls': 0,
            'total_duration': 0,
            'avg_duration': 0,
            'max_duration': 0,
            'min_duration': float('inf'),
            'slow_queries': 0
        }
    
    stats = _query_stats[query_hash]
    stats['total_calls'] += 1
    stats['total_duration'] += duration
    stats['avg_duration'] = stats['total_duration'] / stats['total_calls']
    stats['max_duration'] = max(stats['max_duration'], duration)
    stats['min_duration'] = min(stats['min_duration'], duration)
    
    if duration > _slow_query_threshold:
        stats['slow_queries'] += 1
        logger.warning(
            f"[SLOW_QUERY] {query_type} took {duration:.3f}s (hash: {query_hash}) "
            f"- rows: {row_count if row_count is not None else 'N/A'}"
        )

def get_query_performance_stats() -> Dict[str, Any]:
    """Get comprehensive database query performance statistics"""
    total_calls = sum(stats['total_calls'] for stats in _query_stats.values())
    total_duration = sum(stats['total_duration'] for stats in _query_stats.values())
    total_slow_queries = sum(stats['slow_queries'] for stats in _query_stats.values())
    
    return {
        'summary': {
            'total_queries': total_calls,
            'total_duration': total_duration,
            'average_duration': total_duration / total_calls if total_calls > 0 else 0,
            'slow_queries': total_slow_queries,
            'slow_query_percentage': (total_slow_queries / total_calls * 100) if total_calls > 0 else 0,
            'query_types': len(_query_stats)
        },
        'detailed_stats': _query_stats,
        'top_slow_queries': sorted(
            _query_stats.items(),
            key=lambda x: x[1]['slow_queries'],
            reverse=True
        )[:10]
    }

def log_db_operation(operation_type: str, query: str, params: tuple, duration: float = None, row_count: int = None, error: Exception = None):
    """Comprehensive database operation logging"""
    sanitized_query = sanitize_query_for_log(query)
    
    if error:
        logger.error(
            f"[DB_ERROR] {operation_type} failed - Query: {sanitized_query} "
            f"- Error: {error}"
        )
    else:
        if duration is not None:
            logger.info(
                f"[DB_SUCCESS] {operation_type} completed in {duration:.3f}s - "
                f"Query: {sanitized_query}"
                f"{f' - Rows: {row_count}' if row_count is not None else ''}"
            )
        else:
            logger.info(f"[DB_SUCCESS] {operation_type} - Query: {sanitized_query}")

def demo_query_sanitization():
    """Demonstrate query sanitization for secure logging"""
    print_section("Query Sanitization Demo")
    
    # Test cases for query sanitization
    test_queries = [
        ("Password Query", "UPDATE users SET password='secret123' WHERE email='user@example.com'"),
        ("Token Query", "SELECT * FROM sessions WHERE token='abc123def456' AND active=true"),
        ("API Key Query", "INSERT INTO api_keys (key) VALUES ('sk_test_123456789')"),
        ("Long Query", "SELECT " + ", ".join([f"column_{i}" for i in range(30)]) + " FROM large_table"),
        ("Safe Query", "SELECT id, name FROM users WHERE active=true"),
    ]
    
    print("ğŸ”’ Testing query sanitization for secure logging:")
    for name, query in test_queries:
        sanitized = sanitize_query_for_log(query, max_length=120)
        print(f"\n   {name}:")
        print(f"     Original: {query[:60]}...")
        print(f"     Sanitized: {sanitized}")
        
        # Verify sensitive data is removed
        if "password" in query.lower() or "token" in query.lower() or "key" in query.lower():
            sensitive_words = ["secret123", "abc123def456", "sk_test_123456789"]
            for word in sensitive_words:
                if word in query and word not in sanitized:
                    print(f"     âœ… Sensitive data '{word}' properly removed")

def demo_performance_monitoring():
    """Demonstrate query performance monitoring"""
    print_section("Query Performance Monitoring Demo")
    
    # Reset stats for clean demo
    global _query_stats
    _query_stats = {}
    
    print("âš¡ Simulating various query performance patterns:")
    
    # Simulate different types of queries with various performance characteristics
    query_simulations = [
        ("Fast user lookup", "SELECT id, name FROM users WHERE email=%s", (0.005, 1)),
        ("Alert search", "SELECT * FROM alerts WHERE country=%s AND published > %s", (0.025, 15)),
        ("Bulk data insert", "INSERT INTO raw_alerts (uuid, title, ...) VALUES %s", (0.150, None)),
        ("Complex analytics", "SELECT country, COUNT(*) FROM alerts GROUP BY country", (0.350, 50)),
        ("Slow report query", "SELECT * FROM alerts JOIN raw_alerts ON ... WHERE ...", (1.8, 1000)),
        ("Another user lookup", "SELECT id, name FROM users WHERE email=%s", (0.003, 1)),  # Same query type
        ("Database maintenance", "VACUUM ANALYZE alerts", (2.5, None)),  # Very slow
    ]
    
    for query_name, query, (duration, row_count) in query_simulations:
        print(f"   Executing: {query_name} (Duration: {duration}s, Rows: {row_count})")
        log_query_performance(query, (), duration, row_count)
        time.sleep(0.05)  # Small delay for realism
    
    print(f"\nğŸ“ˆ Performance Statistics Analysis:")
    stats = get_query_performance_stats()
    summary = stats['summary']
    
    print(f"   Total Queries: {summary['total_queries']}")
    print(f"   Total Duration: {summary['total_duration']:.3f}s")
    print(f"   Average Duration: {summary['average_duration']:.3f}s")
    print(f"   Slow Queries: {summary['slow_queries']} ({summary['slow_query_percentage']:.1f}%)")
    print(f"   Unique Query Types: {summary['query_types']}")
    
    # Show detailed stats for slow queries
    if stats['top_slow_queries']:
        print(f"\nğŸŒ Top Slow Query Types:")
        for query_hash, query_stats in stats['top_slow_queries'][:3]:
            if query_stats['slow_queries'] > 0:
                print(f"   {query_stats['query_type']} (Hash: {query_hash}):")
                print(f"     Slow queries: {query_stats['slow_queries']}")
                print(f"     Max duration: {query_stats['max_duration']:.3f}s")
                print(f"     Average: {query_stats['avg_duration']:.3f}s")

def demo_database_logging():
    """Demonstrate enhanced database operation logging"""
    print_section("Enhanced Database Logging Demo")
    
    print("ğŸ“ Testing comprehensive database operation logging:")
    
    # Simulate successful operations
    print("\n   âœ… Successful Operations:")
    log_db_operation(
        operation_type="FETCH_ALL",
        query="SELECT uuid, title, country FROM alerts WHERE published > %s",
        params=("2025-01-01",),
        duration=0.045,
        row_count=25
    )
    
    log_db_operation(
        operation_type="EXECUTE",
        query="INSERT INTO user_usage (email, chat_messages_used) VALUES (%s, %s)",
        params=("user@example.com", 5),
        duration=0.008
    )
    
    # Simulate error operations
    print("\n   âŒ Error Operations:")
    log_db_operation(
        operation_type="FETCH_ONE",
        query="SELECT * FROM non_existent_table WHERE id=%s",
        params=(123,),
        error=Exception("relation 'non_existent_table' does not exist")
    )
    
    log_db_operation(
        operation_type="EXECUTE",
        query="UPDATE users SET password='newpass123' WHERE email=%s",
        params=("nonexistent@example.com",),
        error=Exception("constraint violation: email not found")
    )

def demo_index_optimization():
    """Demonstrate index optimization principles and analysis"""
    print_section("Index Optimization Analysis Demo")
    
    print("ğŸ“Š Analyzing index usage patterns and optimization opportunities:")
    
    # Simulated index analysis results
    index_analysis = [
        {
            'table_name': 'alerts',
            'index_name': 'idx_alerts_published_desc',
            'index_scans': 15420,
            'sequential_scans': 45,
            'efficiency': 'HIGH EFFICIENCY',
            'size': '2.1 MB',
            'recommendation': 'Excellent performance - frequently used for time-based queries'
        },
        {
            'table_name': 'alerts', 
            'index_name': 'idx_alerts_geo_published',
            'index_scans': 8945,
            'sequential_scans': 120,
            'efficiency': 'HIGH EFFICIENCY',
            'size': '3.4 MB',
            'recommendation': 'Composite index working well for geographic + time queries'
        },
        {
            'table_name': 'raw_alerts',
            'index_name': 'idx_raw_alerts_country',
            'index_scans': 234,
            'sequential_scans': 1890,
            'efficiency': 'LOW USAGE',
            'size': '890 KB',
            'recommendation': 'Consider composite index with published date for better performance'
        },
        {
            'table_name': 'users',
            'index_name': 'idx_users_old_column',
            'index_scans': 0,
            'sequential_scans': 1250,
            'efficiency': 'UNUSED',
            'size': '125 KB',
            'recommendation': 'Consider dropping - no usage detected, consuming unnecessary space'
        },
        {
            'table_name': 'alerts',
            'index_name': 'idx_alerts_category_published',
            'index_scans': 3456,
            'sequential_scans': 67,
            'efficiency': 'GOOD USAGE',
            'size': '1.8 MB',
            'recommendation': 'Well-utilized for category-based filtering'
        }
    ]
    
    print("\n   ğŸ“ˆ Index Performance Analysis:")
    for idx in index_analysis:
        efficiency_emoji = {
            'HIGH EFFICIENCY': 'ğŸŸ¢',
            'GOOD USAGE': 'ğŸ’š',
            'LOW USAGE': 'ğŸŸ¡',
            'UNUSED': 'ğŸ”´'
        }.get(idx['efficiency'], 'â“')
        
        print(f"\n   {efficiency_emoji} {idx['table_name']}.{idx['index_name']}:")
        print(f"     Size: {idx['size']}")
        print(f"     Index Scans: {idx['index_scans']:,}")
        print(f"     Sequential Scans: {idx['sequential_scans']:,}")
        print(f"     Efficiency: {idx['efficiency']}")
        print(f"     ğŸ’¡ {idx['recommendation']}")
    
    # Index optimization recommendations
    print(f"\n   ğŸ“‹ Key Index Optimization Strategies:")
    strategies = [
        "Create composite indexes for multi-column WHERE clauses",
        "Use DESC ordering for indexes supporting ORDER BY ... DESC queries",
        "Implement partial indexes with WHERE clauses for filtered queries",
        "Use GIN indexes for JSONB columns and array searches",
        "Monitor index usage statistics regularly and drop unused indexes",
        "Consider covering indexes to avoid table lookups",
        "Use CONCURRENTLY for index creation in production to avoid locks"
    ]
    
    for i, strategy in enumerate(strategies, 1):
        print(f"     {i}. {strategy}")

def demo_database_health_monitoring():
    """Demonstrate database health monitoring concepts"""
    print_section("Database Health Monitoring Demo")
    
    print("ğŸ¥ Demonstrating database health assessment framework:")
    
    # Simulate health metrics for different scenarios
    health_scenarios = [
        {
            'name': 'Healthy Database',
            'metrics': {
                'total_connections': 12,
                'active_connections': 6,
                'cache_hit_ratio': 96.8,
                'index_usage_ratio': 91.5,
                'table_bloat_percentage': 4.2,
                'slow_queries_last_hour': 1,
                'longest_running_query': 25.0,
                'database_size': '2.1 GB'
            },
            'status': 'healthy',
            'recommendations': []
        },
        {
            'name': 'Database with Warning Signs',
            'metrics': {
                'total_connections': 45,
                'active_connections': 22,
                'cache_hit_ratio': 87.2,
                'index_usage_ratio': 76.8,
                'table_bloat_percentage': 16.5,
                'slow_queries_last_hour': 8,
                'longest_running_query': 145.0,
                'database_size': '3.7 GB'
            },
            'status': 'warning',
            'recommendations': [
                'Consider connection pooling to manage high connection count',
                'Review query patterns - index usage below optimal',
                'Schedule vacuum maintenance for tables with high bloat'
            ]
        },
        {
            'name': 'Database with Critical Issues',
            'metrics': {
                'total_connections': 78,
                'active_connections': 45,
                'cache_hit_ratio': 72.1,
                'index_usage_ratio': 58.3,
                'table_bloat_percentage': 28.9,
                'slow_queries_last_hour': 23,
                'longest_running_query': 456.0,
                'database_size': '5.2 GB'
            },
            'status': 'critical',
            'recommendations': [
                'URGENT: Investigate long-running queries causing potential locks',
                'CRITICAL: Cache hit ratio too low - increase shared_buffers',
                'IMMEDIATE: Multiple tables need vacuum - schedule maintenance',
                'Review and optimize slow queries causing performance degradation',
                'Implement connection pooling immediately'
            ]
        }
    ]
    
    for scenario in health_scenarios:
        status_emoji = {'healthy': 'ğŸ’š', 'warning': 'ğŸŸ¡', 'critical': 'ğŸ”´'}
        print(f"\n   {status_emoji[scenario['status']]} {scenario['name']} ({scenario['status'].upper()}):")
        
        metrics = scenario['metrics']
        print(f"     Connections: {metrics['active_connections']}/{metrics['total_connections']}")
        print(f"     Cache Hit Ratio: {metrics['cache_hit_ratio']:.1f}%")
        print(f"     Index Usage: {metrics['index_usage_ratio']:.1f}%")
        print(f"     Table Bloat: {metrics['table_bloat_percentage']:.1f}%")
        print(f"     Slow Queries/Hour: {metrics['slow_queries_last_hour']}")
        print(f"     Longest Query: {metrics['longest_running_query']:.1f}s")
        print(f"     Database Size: {metrics['database_size']}")
        
        if scenario['recommendations']:
            print(f"     ğŸ’¡ Recommendations:")
            for rec in scenario['recommendations']:
                urgency = ''
                if 'URGENT' in rec or 'CRITICAL' in rec or 'IMMEDIATE' in rec:
                    urgency = 'ğŸš¨ '
                print(f"       {urgency}â€¢ {rec}")

def demo_optimization_sql_examples():
    """Demonstrate SQL optimization examples"""
    print_section("SQL Query Optimization Examples")
    
    print("ğŸ”§ Common query optimization patterns for Sentinel AI:")
    
    optimization_examples = [
        {
            'scenario': 'Slow Geographic Alert Queries',
            'before': '''
SELECT * FROM alerts 
WHERE (country = 'Nigeria' OR city = 'Lagos' OR region = 'West Africa')
AND published > NOW() - INTERVAL '7 days'
ORDER BY published DESC
LIMIT 20
            ''',
            'after': '''
-- Optimized with composite index: idx_alerts_geo_published
SELECT uuid, title, summary, country, city, published, threat_level
FROM alerts 
WHERE country = 'Nigeria' 
AND published > NOW() - INTERVAL '7 days'
ORDER BY published DESC
LIMIT 20
            ''',
            'improvements': [
                'Use specific column selection instead of SELECT *',
                'Prioritize exact country match over OR conditions',
                'Composite index on (country, published DESC) for optimal performance'
            ]
        },
        {
            'scenario': 'Inefficient Category Filtering',
            'before': '''
SELECT * FROM alerts 
WHERE category LIKE '%security%' 
OR category LIKE '%cyber%'
ORDER BY score DESC
            ''',
            'after': '''
-- Optimized with normalized categories and proper indexing
SELECT uuid, title, category, score, published
FROM alerts 
WHERE category IN ('cybersecurity', 'physical_security', 'data_security')
AND score > 50
ORDER BY score DESC, published DESC
LIMIT 100
            ''',
            'improvements': [
                'Replace LIKE queries with exact IN matching',
                'Add score threshold to reduce result set',
                'Index on (score DESC, published DESC) for sorting'
            ]
        },
        {
            'scenario': 'Slow JSONB Queries',
            'before': '''
SELECT * FROM alerts 
WHERE tags::text LIKE '%terrorism%'
            ''',
            'after': '''
-- Optimized with GIN index on tags array
SELECT uuid, title, tags, published
FROM alerts 
WHERE 'terrorism' = ANY(tags)
AND published > NOW() - INTERVAL '30 days'
            ''',
            'improvements': [
                'Use array operators instead of text casting',
                'GIN index on tags array for efficient searching',
                'Add time filter to limit search scope'
            ]
        }
    ]
    
    for i, example in enumerate(optimization_examples, 1):
        print(f"\n   ğŸ“ Example {i}: {example['scenario']}")
        print(f"     âŒ Before (Inefficient):")
        for line in example['before'].strip().split('\n'):
            if line.strip():
                print(f"       {line}")
        
        print(f"     âœ… After (Optimized):")
        for line in example['after'].strip().split('\n'):
            if line.strip():
                print(f"       {line}")
        
        print(f"     ğŸ’¡ Key Improvements:")
        for improvement in example['improvements']:
            print(f"       â€¢ {improvement}")

def demo_performance_summary():
    """Demonstrate comprehensive performance summary"""
    print_section("Performance Summary and Recommendations")
    
    print("ğŸ“Š Comprehensive database performance analysis:")
    
    # Use accumulated stats from previous demos
    stats = get_query_performance_stats()
    summary = stats['summary']
    
    print(f"\n   ğŸ“ˆ Query Performance Summary:")
    print(f"     Total Queries Analyzed: {summary['total_queries']:,}")
    print(f"     Total Execution Time: {summary['total_duration']:.3f}s")
    print(f"     Average Query Time: {summary['average_duration']:.3f}s")
    print(f"     Slow Queries: {summary['slow_queries']} ({summary['slow_query_percentage']:.1f}%)")
    print(f"     Unique Query Patterns: {summary['query_types']}")
    
    # Performance assessment
    if summary['slow_query_percentage'] > 20:
        status = "ğŸ”´ CRITICAL"
        assessment = "High percentage of slow queries detected"
    elif summary['slow_query_percentage'] > 10:
        status = "ğŸŸ¡ WARNING"  
        assessment = "Moderate slow query activity"
    else:
        status = "ğŸ’š HEALTHY"
        assessment = "Query performance within acceptable limits"
    
    print(f"\n   {status} Overall Performance: {assessment}")
    
    print(f"\n   ğŸ¯ Key Performance Recommendations:")
    recommendations = [
        "Implement query performance monitoring in production",
        "Set up automated alerting for slow queries (>1s threshold)",
        "Regular analysis of index usage statistics",
        "Implement connection pooling for high-traffic scenarios", 
        "Schedule regular database maintenance (VACUUM, ANALYZE)",
        "Monitor cache hit ratios and adjust memory settings as needed",
        "Create composite indexes for common query patterns"
    ]
    
    for i, rec in enumerate(recommendations, 1):
        print(f"     {i}. {rec}")

def main():
    """Run comprehensive enhanced database operations demo"""
    
    print_header("Enhanced Database Operations - Standalone Demo")
    print("ğŸš€ Demonstrating database optimization, monitoring, and logging concepts")
    print("=" * 80)
    
    try:
        # Run all demo sections
        demo_query_sanitization()
        demo_performance_monitoring()
        demo_database_logging()
        demo_index_optimization()
        demo_database_health_monitoring()
        demo_optimization_sql_examples()
        demo_performance_summary()
        
        print_header("Demo Summary & Implementation Guide")
        print("âœ… All enhanced database features demonstrated successfully!")
        
        print("\nğŸ“‹ Key Features Implemented:")
        features = [
            "ğŸ”’ Query sanitization for secure logging (prevents credential leakage)",
            "âš¡ Real-time query performance monitoring with statistics",
            "ğŸ“ Comprehensive database operation logging with context",
            "ğŸ¥ Database health assessment framework with recommendations",
            "ğŸ” Index optimization analysis and efficiency tracking",
            "ğŸ“Š Performance statistics aggregation and reporting",
            "ğŸ¯ Automated optimization recommendations based on metrics",
            "ğŸ›¡ï¸ Security-aware logging that protects sensitive data"
        ]
        
        for feature in features:
            print(f"   {feature}")
        
        print(f"\nğŸ’¡ Database Optimization Benefits:")
        benefits = [
            "Enhanced visibility into query performance and bottlenecks",
            "Automated detection of slow queries and optimization opportunities", 
            "Secure logging that protects passwords, tokens, and sensitive data",
            "Real-time monitoring of database health and resource usage",
            "Proactive recommendations for index optimization",
            "Comprehensive audit trail for database operations",
            "Performance trend analysis for capacity planning"
        ]
        
        for benefit in benefits:
            print(f"   â€¢ {benefit}")
        
        print(f"\nğŸš€ Implementation Steps:")
        steps = [
            "Apply the enhanced db_utils.py with logging functions",
            "Run database_index_optimization.sql to create optimal indexes",
            "Deploy database_monitor.py for health monitoring",
            "Configure slow query alerting thresholds (default: 1.0s)",
            "Set up regular performance review processes",
            "Implement automated database maintenance schedules"
        ]
        
        for i, step in enumerate(steps, 1):
            print(f"   {i}. {step}")
        
        print(f"\nğŸ“Š Performance Impact Expected:")
        impact_metrics = [
            "Query response time reduction: 20-40% for indexed queries",
            "Slow query detection: Real-time alerting for >1s queries",
            "Index efficiency: Automated analysis of usage patterns", 
            "Security compliance: Sanitized logging prevents data leaks",
            "Operational visibility: 360Â° view of database health"
        ]
        
        for metric in impact_metrics:
            print(f"   ğŸ“ˆ {metric}")
        
    except Exception as e:
        print(f"âŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print(f"\nğŸ¬ Demo completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

if __name__ == "__main__":
    main()
